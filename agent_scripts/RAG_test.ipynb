{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin integrating RAG mechanism with Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cpp Lambda w/ Python bindings\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n",
    "    filename=\"*q8_0.gguf\",\n",
    "    verbose=False\n",
    ")\n",
    "# llm = Llama(\n",
    "#       model_path=\"./models/7B/llama-model.gguf\",\n",
    "#       # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "#       # seed=1337, # Uncomment to set a specific seed\n",
    "#       # n_ctx=2048, # Uncomment to increase the context window\n",
    "# )\n",
    "output = llm(\n",
    "    #   \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      \"Q: Teach me how to write a simple python code A:\", # Prompt\n",
    "      max_tokens=1000, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\",\"@\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
